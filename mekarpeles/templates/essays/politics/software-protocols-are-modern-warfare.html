<iframe src="https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fmichael.karpeles%2Fposts%2F10103483448772600&width=500" width="500" height="379" style="border:none;overflow:hidden" scrolling="no" frameborder="0" allowTransparency="true" allow="encrypted-media"></iframe>

<h2>Follow-up</h2>
<p><a href="https://www.facebook.com/jackson.kernion/posts/10156426135140132?comment_id=10156427320445132&comment_tracking=%7B%22tn%22%3A%22R%22%7D">Follow-up Conversation with Jackson Kernion</a></p>

<p>Jackson writes:</p>
<quote>
<p>Credit scores, but for online behavior.</p>

<p>So: you try to sign up for some online service, but it checks your score first. Maybe some services only allow you to create an account if you have a score above a certain threshold. Or maybe the service unlocks different priveleges/features depending on your score.</p>

<p>If you 'misbehave' on some service, by being blocked, being reported, or misusing the service, the service can then report back that behavior to the scoring agency. It's up to the scoring agency to determine if/how that purported misbehvior should affect your score.</p>

<p>As I see it, this would modularize and commoditize a key "platform integrity" function, making it cheaper and easier for a broad array of services to secure their platform.</p>

<p>A generic "is this account not a bot/bad actor" score would be widey applicable. But the same basic technology could be used for other kinds of reputation tracking...</p>
</quote>

<p>To which I reply:</p>

<quote>
<p>What you describe is in deployed in practice in China. It's fairly dystopian:</p>
<ul>
<li><a href="https://newyork.cbslocal.com/2018/04/24/china-assigns-every-citizen-a-social-credit-score-to-identify-who-is-and-isnt-trustworthy/">https://newyork.cbslocal.com/2018/04/24/china-assigns-every-citizen-a-social-credit-score-to-identify-who-is-and-isnt-trustworthy/</a></li>
<li><a href="https://www.wired.co.uk/article/china-social-credit">https://www.wired.co.uk/article/china-social-credit</a>
</ul>

<p>The idea, like any technology, is not intrinsically bad. The problem is, such an idea requires protections to prevent individuals from being oppressed by those who control computing power / are controlling flows of resources and access to services.</p>

<p>Corporate interests resist regulation like the plague. And without the right regulations to protect people, this scheme will result in massive disenfranchisement. e.g. In the 80's the way for insurance companies to compete was covering as many people as possible (competing in a zero sum game). In the 2000's, because most people are covered, the strategy has switched to algorithmically reducing liability -- i.e. using computing resources to exclude anyone who is high risk from being covered. It wasn't until recently that the consumer received temporary reprieve via policy which prevents insurance companies from excluding people based on pre-existing conditions. But my guess is (like the 2015 Open Internet Order, established under the same administration) that this policy will also be short lived and succumb to corporate pressure.</p>

<p>The most relevant book on the subject is Jaron Lanier's, "Who Owns the Future" https://openlibrary.org/.../OL16801714W/Who_Owns_the_Future</p>

<p>Tristan Harris describes such scenarios (e.g. those where corporations might access / compute "scores" for people and classify people into buckets) as “a supercomputer [...] playing chess with your mind”. Spoiler alert: you as the patron/consumer don't win.</p>

<p>A potential alternative universe might look like this (pseudonymous identities)
<a href="https://michaelkarpeles.com/essays/philosophy/2040">The Web in 2040</a>
</p>
</quote>






